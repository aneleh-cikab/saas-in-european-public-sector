{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Downloading: 100%|██████████| 6.78M/6.78M [00:00<00:00, 13.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Srcraping tender links\n",
    "\n",
    "The code below is used to dynamically scrape tenders for CPV codes 48000000 and 72000000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting search results\n",
    "\n",
    "## navigating to the page search form\n",
    "driver.get('https://ted.europa.eu/TED/search/search.do')\n",
    "\n",
    "## geting to search results: all dates + cpv selected  codes\n",
    "search_scope = driver.find_element(\"xpath\", '//*[@id=\"searchScope3\"]').click()\n",
    "expert_search = driver.find_element(\"xpath\", '//*[@id=\"switchToExpert\"]').click()\n",
    "search_box = driver.find_element(\"xpath\", '//*[@id=\"expertSearchCriteria.query\"]').send_keys(\"PD=[20000101 <> 20221231] AND PC=[48000000 or 72000000]\")\n",
    "\n",
    "## search click\n",
    "search_button = driver.find_element(\"xpath\", '//*[@id=\"scope-fs\"]/button')\n",
    "driver.execute_script(\"arguments[0].click();\", search_button)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389714\n"
     ]
    }
   ],
   "source": [
    "#getting the number of results to know how many pages to scrape\n",
    "import math\n",
    "\n",
    "results = driver.find_element('xpath','//div[@class = \"pagebanner\"]').text\n",
    "#trim 18 spaces from the left and 7 spaces from the right of no_results\n",
    "results = results[18:]\n",
    "results = results[:-8]\n",
    "#replace , in no. of results with nothing\n",
    "results = int(results.replace(',',''))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping tender links\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "for page in range(1, math.ceil(results/25)+1):\n",
    "    # print page number for every 100 pages\n",
    "    if page % 100 == 0:\n",
    "        print(\"Now at page: \", page)\n",
    "\n",
    "    # get links\n",
    "    elements = driver.find_elements('xpath', '//a[@title=\"View this notice\"]')\n",
    "\n",
    "    # if there are not 25 elements on page, add page to error_page list\n",
    "    if len(elements) != 25:\n",
    "        print(\"Error: not 25 elements on page: \", page)\n",
    "        with open('../data/error_page.txt', 'a') as f:\n",
    "            f.write(str(page) + '\\n')\n",
    "              \n",
    "    #write links to file\n",
    "    for link in elements:\n",
    "        with open('../data/links.txt', 'a') as f:\n",
    "            f.write(link.get_attribute('href') + '\\n')\n",
    "        \n",
    "    # write last page to file\n",
    "    with open('../data/last_page.txt', 'w') as f:\n",
    "        f.write(str(page))\n",
    "    \n",
    "    # sleep\n",
    "    time.sleep(random.uniform(1,2))\n",
    "\n",
    "    #next page\n",
    "    button = driver.find_element('xpath','//div[@title=\"Next\"]/a')\n",
    "    driver.execute_script(\"arguments[0].click();\", button) #to solve ElementClickInterceptedException\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read links from file\n",
    "with open('links.txt', 'r') as f, open('links_2.txt', 'r') as f2:\n",
    "    links = f.read().splitlines()\n",
    "    #read from f2 and add to links\n",
    "    links.extend(f2.read().splitlines())\n",
    "    f.close()\n",
    "\n",
    "# save links in file\n",
    "with open('../data//links-full.txt', 'w') as f:\n",
    "    for link in links:\n",
    "        f.write(link + '\\n')\n",
    "    f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading html documents from obtained links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read links from file\n",
    "with open('../data/links-full.txt', 'r') as f:\n",
    "    links = f.read().splitlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "389714"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating directory for scraped data\n",
    "import os\n",
    "os.mkdir('../data/tender-data-htmls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.request import urlopen\n",
    "import time\n",
    "import random\n",
    "\n",
    "# folder for saving html files\n",
    "folder = '../data/tender-data-htmls//'\n",
    "\n",
    "# saving html files\n",
    "for counter, link in enumerate(links, 1): # starts from 1\n",
    "    if counter < 130000: # scraping distributed across 4 machines\n",
    "\n",
    "        # print counter to keep track of progress\n",
    "        if counter % 1000 == 0:\n",
    "            print(\"Now at link number: \", counter)\n",
    "\n",
    "        # write last link to file \n",
    "        with open('link_counter.txt', 'w') as f:\n",
    "            f.write(str(counter))\n",
    "        \n",
    "        # link name\n",
    "        notice_id = link.split(':')[-4]\n",
    "        name = folder + notice_id + \".html\"\n",
    "            \n",
    "        # download html if file does not exist\n",
    "        # if file exists, skip\n",
    "        if not os.path.isfile(name):\n",
    "            \n",
    "            # get html\n",
    "            page = urlopen(link).read().decode('utf-8')\n",
    "\n",
    "            # save html as name\n",
    "            with open(name, 'w', encoding='utf-8') as f:\n",
    "                f.write(page)\n",
    "\n",
    "        # sleep\n",
    "        time.sleep(random.uniform(1,2))\n",
    "\n",
    "        if counter % 6000 == 0:\n",
    "            print(\"Now at link number: \", counter)\n",
    "            time.sleep(600)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saas-in-eu-NeA8_TIc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f381d90e69f5025c354a600a2cf39ce2857dd0aad78952a7b16a614f9e490453"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
